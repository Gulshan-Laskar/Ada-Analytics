{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_4111_oW7Ck",
        "outputId": "c709a06d-61cb-487d-fe6c-d1534af758ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: praw in c:\\users\\saikr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (7.8.1)\n",
            "Requirement already satisfied: pandas in c:\\users\\saikr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.2.3)\n",
            "Requirement already satisfied: requests in c:\\users\\saikr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.32.3)\n",
            "Requirement already satisfied: prawcore<3,>=2.4 in c:\\users\\saikr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from praw) (2.4.0)\n",
            "Requirement already satisfied: update_checker>=0.18 in c:\\users\\saikr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from praw) (0.18.0)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in c:\\users\\saikr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\saikr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\saikr\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\saikr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\saikr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\saikr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\saikr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\saikr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\saikr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\saikr\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install praw pandas requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDqgR6l1XC7b",
        "outputId": "61454f45-1a31-49da-da41-75a0118069c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 7709 valid tickers.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def load_tickers():\n",
        "    nasdaq = pd.read_csv(\"nasdaq_.csv\")\n",
        "    nyse = pd.read_csv(\"nyse.csv\")\n",
        "\n",
        "    # Make sure the column name is correct; use 'Symbol' or 'ACT Symbol'\n",
        "    if 'Symbol' in nasdaq.columns:\n",
        "        nasdaq_tickers = set(nasdaq['Symbol'].str.upper())\n",
        "    else:\n",
        "        nasdaq_tickers = set(nasdaq['ACT Symbol'].str.upper())\n",
        "\n",
        "    if 'Symbol' in nyse.columns:\n",
        "        nyse_tickers = set(nyse['Symbol'].str.upper())\n",
        "    else:\n",
        "        nyse_tickers = set(nyse['ACT Symbol'].str.upper())\n",
        "\n",
        "    all_tickers = nasdaq_tickers.union(nyse_tickers)\n",
        "    return all_tickers\n",
        "\n",
        "# Load and print count\n",
        "valid_tickers = load_tickers()\n",
        "print(f\"Loaded {len(valid_tickers)} valid tickers.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jLm0CWGsbOVu"
      },
      "outputs": [],
      "source": [
        "import praw\n",
        "import re\n",
        "from collections import Counter\n",
        "import time\n",
        "\n",
        "reddit = praw.Reddit(\n",
        "    client_id=\"Lk7amzjYlHw4NZN4jVeMOA\",\n",
        "    client_secret=\"4Xv7mFZcXHbcvUkQG98tMau-BvEFFg\",\n",
        "    user_agent=\"WSB_Sentiment_Bot/1.0 by Defiant-Fee-533\",\n",
        "    check_for_async=False\n",
        ")\n",
        "\n",
        "subreddits = ['wallstreetbets', 'stocks', 'investing']\n",
        "post_limit = 200  # number of posts to scan per subreddit\n",
        "\n",
        "# Regex to extract uppercase words 1-5 letters (possible ticker symbols)\n",
        "ticker_candidate_pattern = re.compile(r'\\b[A-Z]{1,5}\\b')\n",
        "\n",
        "def find_trending_tickers():\n",
        "    all_candidates = []\n",
        "\n",
        "    for subreddit_name in subreddits:\n",
        "        print(f\"Scanning r/{subreddit_name}...\")\n",
        "        subreddit = reddit.subreddit(subreddit_name)\n",
        "        for post in subreddit.hot(limit=post_limit):\n",
        "            text = post.title.upper()\n",
        "            candidates = ticker_candidate_pattern.findall(text)\n",
        "            all_candidates.extend(candidates)\n",
        "            time.sleep(0.1)  # polite pause to avoid rate limit\n",
        "\n",
        "    # Filter candidates by whether they are valid tickers\n",
        "    filtered = [t for t in all_candidates if t in valid_tickers]\n",
        "\n",
        "    # Count frequencies and get top 20 trending tickers\n",
        "    counter = Counter(filtered)\n",
        "    top_20 = counter.most_common(20)\n",
        "\n",
        "    print(\"\\nTop 20 trending tickers detected on Reddit:\")\n",
        "    for ticker, count in top_20:\n",
        "        print(f\"{ticker}: {count} mentions\")\n",
        "\n",
        "    return [t[0] for t in top_20]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isR7OJZqXHQ7",
        "outputId": "761d9329-7240-49ba-b321-9bec022c9817"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scanning r/wallstreetbets...\n",
            "Scanning r/stocks...\n",
            "Scanning r/investing...\n",
            "\n",
            "Top 20 trending tickers detected on Reddit:\n",
            "A: 84 mentions\n",
            "FOR: 67 mentions\n",
            "ON: 67 mentions\n",
            "S: 62 mentions\n",
            "YOU: 48 mentions\n",
            "OR: 34 mentions\n",
            "IT: 34 mentions\n",
            "ARE: 29 mentions\n",
            "TSLA: 26 mentions\n",
            "AS: 23 mentions\n",
            "JUNE: 20 mentions\n",
            "GOOD: 20 mentions\n",
            "AI: 20 mentions\n",
            "UP: 18 mentions\n",
            "ALL: 17 mentions\n",
            "NOW: 16 mentions\n",
            "NEXT: 15 mentions\n",
            "CAN: 14 mentions\n",
            "BE: 14 mentions\n",
            "M: 14 mentions\n",
            "\n",
            "Fetching posts for A...\n",
            "Fetched 15 posts for A\n",
            "\n",
            "Fetching posts for FOR...\n",
            "Fetched 15 posts for FOR\n",
            "\n",
            "Fetching posts for ON...\n",
            "Fetched 15 posts for ON\n",
            "\n",
            "Fetching posts for S...\n",
            "Fetched 15 posts for S\n",
            "\n",
            "Fetching posts for YOU...\n",
            "Fetched 15 posts for YOU\n",
            "\n",
            "Fetching posts for OR...\n",
            "Fetched 15 posts for OR\n",
            "\n",
            "Fetching posts for IT...\n",
            "Fetched 15 posts for IT\n",
            "\n",
            "Fetching posts for ARE...\n",
            "Fetched 15 posts for ARE\n",
            "\n",
            "Fetching posts for TSLA...\n",
            "Fetched 15 posts for TSLA\n",
            "\n",
            "Fetching posts for AS...\n",
            "Fetched 15 posts for AS\n",
            "\n",
            "Fetching posts for JUNE...\n",
            "Fetched 15 posts for JUNE\n",
            "\n",
            "Fetching posts for GOOD...\n",
            "Fetched 15 posts for GOOD\n",
            "\n",
            "Fetching posts for AI...\n",
            "Fetched 15 posts for AI\n",
            "\n",
            "Fetching posts for UP...\n",
            "Fetched 15 posts for UP\n",
            "\n",
            "Fetching posts for ALL...\n",
            "Fetched 15 posts for ALL\n",
            "\n",
            "Fetching posts for NOW...\n",
            "Fetched 15 posts for NOW\n",
            "\n",
            "Fetching posts for NEXT...\n",
            "Fetched 15 posts for NEXT\n",
            "\n",
            "Fetching posts for CAN...\n",
            "Fetched 15 posts for CAN\n",
            "\n",
            "Fetching posts for BE...\n",
            "Fetched 15 posts for BE\n",
            "\n",
            "Fetching posts for M...\n",
            "Fetched 15 posts for M\n",
            "\n",
            "Saved extracted data to 'reddit_trending_tickers_posts.csv'\n"
          ]
        }
      ],
      "source": [
        "def get_posts_for_ticker(ticker, max_posts=15):\n",
        "    posts = []\n",
        "    query = f'title:{ticker} OR selftext:{ticker}'\n",
        "\n",
        "    for subreddit_name in subreddits:\n",
        "        subreddit = reddit.subreddit(subreddit_name)\n",
        "        for post in subreddit.search(query, sort='new', limit=max_posts):\n",
        "            posts.append({\n",
        "                'ticker': ticker,\n",
        "                'post_id': post.id,\n",
        "                'title': post.title,\n",
        "                'selftext': post.selftext,\n",
        "                'url': post.url,\n",
        "                'created_utc': post.created_utc,\n",
        "                'score': post.score,\n",
        "                'num_comments': post.num_comments,\n",
        "                'subreddit': subreddit_name\n",
        "            })\n",
        "            if len(posts) >= max_posts:\n",
        "                break\n",
        "        if len(posts) >= max_posts:\n",
        "            break\n",
        "    return posts\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    valid_tickers = load_tickers()\n",
        "    trending_tickers = find_trending_tickers()\n",
        "\n",
        "    all_posts = []\n",
        "    for ticker in trending_tickers:\n",
        "        print(f\"\\nFetching posts for {ticker}...\")\n",
        "        posts = get_posts_for_ticker(ticker)\n",
        "        all_posts.extend(posts)\n",
        "        print(f\"Fetched {len(posts)} posts for {ticker}\")\n",
        "        time.sleep(1)\n",
        "    # Convert the list of posts to a DataFrame\n",
        "    df = pd.DataFrame(all_posts)\n",
        "    # Save the DataFrame to a CSV file\n",
        "    output_file = \"reddit_trending_tickers_posts.csv\"\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(f\"\\nSaved extracted data to '{output_file}'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEEUc6n9d4ZI",
        "outputId": "f4d8f752-75ae-4f01-92ea-c00705886757"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.11/dist-packages (0.2.61)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.0.2)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.32.3)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.3.8)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2025.2)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance) (3.18.1)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.13.4)\n",
            "Requirement already satisfied: curl_cffi>=0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.10.0)\n",
            "Requirement already satisfied: protobuf>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (5.29.4)\n",
            "Requirement already satisfied: websockets>=13.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (15.0.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (4.13.2)\n",
            "Requirement already satisfied: cffi>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from curl_cffi>=0.7->yfinance) (1.17.1)\n",
            "Requirement already satisfied: certifi>=2024.2.2 in /usr/local/lib/python3.11/dist-packages (from curl_cffi>=0.7->yfinance) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->yfinance) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2.4.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "pip install yfinance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYu6X4xhmCRP",
        "outputId": "e3cd22f7-096c-485d-959c-1d84e3085024"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['MMM', 'AOS', 'ABT', 'ABBV', 'ACN', 'ADBE', 'AMD', 'AES', 'AFL', 'A']\n"
          ]
        }
      ],
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "\n",
        "def get_sp500_tickers():\n",
        "    # Fetch S&P 500 tickers\n",
        "    sp500 = yf.Ticker(\"^GSPC\")\n",
        "\n",
        "    # yfinance does not have a direct method for tickers list,\n",
        "    # but there's a popular workaround to get the list via Wikipedia\n",
        "    sp500_table = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
        "    sp500_df = sp500_table[0]\n",
        "\n",
        "    # Get the Symbol column as a list\n",
        "    tickers = sp500_df['Symbol'].tolist()\n",
        "    # Make uppercase for consistency\n",
        "    tickers = [t.upper() for t in tickers]\n",
        "\n",
        "    return tickers\n",
        "\n",
        "# Example usage\n",
        "valid_tickers = get_sp500_tickers()\n",
        "print(valid_tickers[:10])  # print first 10 tickers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pNTTFiezmHrS",
        "outputId": "8d0d1989-fb8a-4b2c-a39c-a0641d487a42"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['MMM',\n",
              " 'AOS',\n",
              " 'ABT',\n",
              " 'ABBV',\n",
              " 'ACN',\n",
              " 'ADBE',\n",
              " 'AMD',\n",
              " 'AES',\n",
              " 'AFL',\n",
              " 'A',\n",
              " 'APD',\n",
              " 'ABNB',\n",
              " 'AKAM',\n",
              " 'ALB',\n",
              " 'ARE',\n",
              " 'ALGN',\n",
              " 'ALLE',\n",
              " 'LNT',\n",
              " 'ALL',\n",
              " 'GOOGL',\n",
              " 'GOOG',\n",
              " 'MO',\n",
              " 'AMZN',\n",
              " 'AMCR',\n",
              " 'AEE',\n",
              " 'AEP',\n",
              " 'AXP',\n",
              " 'AIG',\n",
              " 'AMT',\n",
              " 'AWK',\n",
              " 'AMP',\n",
              " 'AME',\n",
              " 'AMGN',\n",
              " 'APH',\n",
              " 'ADI',\n",
              " 'ANSS',\n",
              " 'AON',\n",
              " 'APA',\n",
              " 'APO',\n",
              " 'AAPL',\n",
              " 'AMAT',\n",
              " 'APTV',\n",
              " 'ACGL',\n",
              " 'ADM',\n",
              " 'ANET',\n",
              " 'AJG',\n",
              " 'AIZ',\n",
              " 'T',\n",
              " 'ATO',\n",
              " 'ADSK',\n",
              " 'ADP',\n",
              " 'AZO',\n",
              " 'AVB',\n",
              " 'AVY',\n",
              " 'AXON',\n",
              " 'BKR',\n",
              " 'BALL',\n",
              " 'BAC',\n",
              " 'BAX',\n",
              " 'BDX',\n",
              " 'BRK.B',\n",
              " 'BBY',\n",
              " 'TECH',\n",
              " 'BIIB',\n",
              " 'BLK',\n",
              " 'BX',\n",
              " 'BK',\n",
              " 'BA',\n",
              " 'BKNG',\n",
              " 'BSX',\n",
              " 'BMY',\n",
              " 'AVGO',\n",
              " 'BR',\n",
              " 'BRO',\n",
              " 'BF.B',\n",
              " 'BLDR',\n",
              " 'BG',\n",
              " 'BXP',\n",
              " 'CHRW',\n",
              " 'CDNS',\n",
              " 'CZR',\n",
              " 'CPT',\n",
              " 'CPB',\n",
              " 'COF',\n",
              " 'CAH',\n",
              " 'KMX',\n",
              " 'CCL',\n",
              " 'CARR',\n",
              " 'CAT',\n",
              " 'CBOE',\n",
              " 'CBRE',\n",
              " 'CDW',\n",
              " 'COR',\n",
              " 'CNC',\n",
              " 'CNP',\n",
              " 'CF',\n",
              " 'CRL',\n",
              " 'SCHW',\n",
              " 'CHTR',\n",
              " 'CVX',\n",
              " 'CMG',\n",
              " 'CB',\n",
              " 'CHD',\n",
              " 'CI',\n",
              " 'CINF',\n",
              " 'CTAS',\n",
              " 'CSCO',\n",
              " 'C',\n",
              " 'CFG',\n",
              " 'CLX',\n",
              " 'CME',\n",
              " 'CMS',\n",
              " 'KO',\n",
              " 'CTSH',\n",
              " 'COIN',\n",
              " 'CL',\n",
              " 'CMCSA',\n",
              " 'CAG',\n",
              " 'COP',\n",
              " 'ED',\n",
              " 'STZ',\n",
              " 'CEG',\n",
              " 'COO',\n",
              " 'CPRT',\n",
              " 'GLW',\n",
              " 'CPAY',\n",
              " 'CTVA',\n",
              " 'CSGP',\n",
              " 'COST',\n",
              " 'CTRA',\n",
              " 'CRWD',\n",
              " 'CCI',\n",
              " 'CSX',\n",
              " 'CMI',\n",
              " 'CVS',\n",
              " 'DHR',\n",
              " 'DRI',\n",
              " 'DVA',\n",
              " 'DAY',\n",
              " 'DECK',\n",
              " 'DE',\n",
              " 'DELL',\n",
              " 'DAL',\n",
              " 'DVN',\n",
              " 'DXCM',\n",
              " 'FANG',\n",
              " 'DLR',\n",
              " 'DG',\n",
              " 'DLTR',\n",
              " 'D',\n",
              " 'DPZ',\n",
              " 'DASH',\n",
              " 'DOV',\n",
              " 'DOW',\n",
              " 'DHI',\n",
              " 'DTE',\n",
              " 'DUK',\n",
              " 'DD',\n",
              " 'EMN',\n",
              " 'ETN',\n",
              " 'EBAY',\n",
              " 'ECL',\n",
              " 'EIX',\n",
              " 'EW',\n",
              " 'EA',\n",
              " 'ELV',\n",
              " 'EMR',\n",
              " 'ENPH',\n",
              " 'ETR',\n",
              " 'EOG',\n",
              " 'EPAM',\n",
              " 'EQT',\n",
              " 'EFX',\n",
              " 'EQIX',\n",
              " 'EQR',\n",
              " 'ERIE',\n",
              " 'ESS',\n",
              " 'EL',\n",
              " 'EG',\n",
              " 'EVRG',\n",
              " 'ES',\n",
              " 'EXC',\n",
              " 'EXE',\n",
              " 'EXPE',\n",
              " 'EXPD',\n",
              " 'EXR',\n",
              " 'XOM',\n",
              " 'FFIV',\n",
              " 'FDS',\n",
              " 'FICO',\n",
              " 'FAST',\n",
              " 'FRT',\n",
              " 'FDX',\n",
              " 'FIS',\n",
              " 'FITB',\n",
              " 'FSLR',\n",
              " 'FE',\n",
              " 'FI',\n",
              " 'F',\n",
              " 'FTNT',\n",
              " 'FTV',\n",
              " 'FOXA',\n",
              " 'FOX',\n",
              " 'BEN',\n",
              " 'FCX',\n",
              " 'GRMN',\n",
              " 'IT',\n",
              " 'GE',\n",
              " 'GEHC',\n",
              " 'GEV',\n",
              " 'GEN',\n",
              " 'GNRC',\n",
              " 'GD',\n",
              " 'GIS',\n",
              " 'GM',\n",
              " 'GPC',\n",
              " 'GILD',\n",
              " 'GPN',\n",
              " 'GL',\n",
              " 'GDDY',\n",
              " 'GS',\n",
              " 'HAL',\n",
              " 'HIG',\n",
              " 'HAS',\n",
              " 'HCA',\n",
              " 'DOC',\n",
              " 'HSIC',\n",
              " 'HSY',\n",
              " 'HES',\n",
              " 'HPE',\n",
              " 'HLT',\n",
              " 'HOLX',\n",
              " 'HD',\n",
              " 'HON',\n",
              " 'HRL',\n",
              " 'HST',\n",
              " 'HWM',\n",
              " 'HPQ',\n",
              " 'HUBB',\n",
              " 'HUM',\n",
              " 'HBAN',\n",
              " 'HII',\n",
              " 'IBM',\n",
              " 'IEX',\n",
              " 'IDXX',\n",
              " 'ITW',\n",
              " 'INCY',\n",
              " 'IR',\n",
              " 'PODD',\n",
              " 'INTC',\n",
              " 'ICE',\n",
              " 'IFF',\n",
              " 'IP',\n",
              " 'IPG',\n",
              " 'INTU',\n",
              " 'ISRG',\n",
              " 'IVZ',\n",
              " 'INVH',\n",
              " 'IQV',\n",
              " 'IRM',\n",
              " 'JBHT',\n",
              " 'JBL',\n",
              " 'JKHY',\n",
              " 'J',\n",
              " 'JNJ',\n",
              " 'JCI',\n",
              " 'JPM',\n",
              " 'JNPR',\n",
              " 'K',\n",
              " 'KVUE',\n",
              " 'KDP',\n",
              " 'KEY',\n",
              " 'KEYS',\n",
              " 'KMB',\n",
              " 'KIM',\n",
              " 'KMI',\n",
              " 'KKR',\n",
              " 'KLAC',\n",
              " 'KHC',\n",
              " 'KR',\n",
              " 'LHX',\n",
              " 'LH',\n",
              " 'LRCX',\n",
              " 'LW',\n",
              " 'LVS',\n",
              " 'LDOS',\n",
              " 'LEN',\n",
              " 'LII',\n",
              " 'LLY',\n",
              " 'LIN',\n",
              " 'LYV',\n",
              " 'LKQ',\n",
              " 'LMT',\n",
              " 'L',\n",
              " 'LOW',\n",
              " 'LULU',\n",
              " 'LYB',\n",
              " 'MTB',\n",
              " 'MPC',\n",
              " 'MKTX',\n",
              " 'MAR',\n",
              " 'MMC',\n",
              " 'MLM',\n",
              " 'MAS',\n",
              " 'MA',\n",
              " 'MTCH',\n",
              " 'MKC',\n",
              " 'MCD',\n",
              " 'MCK',\n",
              " 'MDT',\n",
              " 'MRK',\n",
              " 'META',\n",
              " 'MET',\n",
              " 'MTD',\n",
              " 'MGM',\n",
              " 'MCHP',\n",
              " 'MU',\n",
              " 'MSFT',\n",
              " 'MAA',\n",
              " 'MRNA',\n",
              " 'MHK',\n",
              " 'MOH',\n",
              " 'TAP',\n",
              " 'MDLZ',\n",
              " 'MPWR',\n",
              " 'MNST',\n",
              " 'MCO',\n",
              " 'MS',\n",
              " 'MOS',\n",
              " 'MSI',\n",
              " 'MSCI',\n",
              " 'NDAQ',\n",
              " 'NTAP',\n",
              " 'NFLX',\n",
              " 'NEM',\n",
              " 'NWSA',\n",
              " 'NWS',\n",
              " 'NEE',\n",
              " 'NKE',\n",
              " 'NI',\n",
              " 'NDSN',\n",
              " 'NSC',\n",
              " 'NTRS',\n",
              " 'NOC',\n",
              " 'NCLH',\n",
              " 'NRG',\n",
              " 'NUE',\n",
              " 'NVDA',\n",
              " 'NVR',\n",
              " 'NXPI',\n",
              " 'ORLY',\n",
              " 'OXY',\n",
              " 'ODFL',\n",
              " 'OMC',\n",
              " 'ON',\n",
              " 'OKE',\n",
              " 'ORCL',\n",
              " 'OTIS',\n",
              " 'PCAR',\n",
              " 'PKG',\n",
              " 'PLTR',\n",
              " 'PANW',\n",
              " 'PARA',\n",
              " 'PH',\n",
              " 'PAYX',\n",
              " 'PAYC',\n",
              " 'PYPL',\n",
              " 'PNR',\n",
              " 'PEP',\n",
              " 'PFE',\n",
              " 'PCG',\n",
              " 'PM',\n",
              " 'PSX',\n",
              " 'PNW',\n",
              " 'PNC',\n",
              " 'POOL',\n",
              " 'PPG',\n",
              " 'PPL',\n",
              " 'PFG',\n",
              " 'PG',\n",
              " 'PGR',\n",
              " 'PLD',\n",
              " 'PRU',\n",
              " 'PEG',\n",
              " 'PTC',\n",
              " 'PSA',\n",
              " 'PHM',\n",
              " 'PWR',\n",
              " 'QCOM',\n",
              " 'DGX',\n",
              " 'RL',\n",
              " 'RJF',\n",
              " 'RTX',\n",
              " 'O',\n",
              " 'REG',\n",
              " 'REGN',\n",
              " 'RF',\n",
              " 'RSG',\n",
              " 'RMD',\n",
              " 'RVTY',\n",
              " 'ROK',\n",
              " 'ROL',\n",
              " 'ROP',\n",
              " 'ROST',\n",
              " 'RCL',\n",
              " 'SPGI',\n",
              " 'CRM',\n",
              " 'SBAC',\n",
              " 'SLB',\n",
              " 'STX',\n",
              " 'SRE',\n",
              " 'NOW',\n",
              " 'SHW',\n",
              " 'SPG',\n",
              " 'SWKS',\n",
              " 'SJM',\n",
              " 'SW',\n",
              " 'SNA',\n",
              " 'SOLV',\n",
              " 'SO',\n",
              " 'LUV',\n",
              " 'SWK',\n",
              " 'SBUX',\n",
              " 'STT',\n",
              " 'STLD',\n",
              " 'STE',\n",
              " 'SYK',\n",
              " 'SMCI',\n",
              " 'SYF',\n",
              " 'SNPS',\n",
              " 'SYY',\n",
              " 'TMUS',\n",
              " 'TROW',\n",
              " 'TTWO',\n",
              " 'TPR',\n",
              " 'TRGP',\n",
              " 'TGT',\n",
              " 'TEL',\n",
              " 'TDY',\n",
              " 'TER',\n",
              " 'TSLA',\n",
              " 'TXN',\n",
              " 'TPL',\n",
              " 'TXT',\n",
              " 'TMO',\n",
              " 'TJX',\n",
              " 'TKO',\n",
              " 'TSCO',\n",
              " 'TT',\n",
              " 'TDG',\n",
              " 'TRV',\n",
              " 'TRMB',\n",
              " 'TFC',\n",
              " 'TYL',\n",
              " 'TSN',\n",
              " 'USB',\n",
              " 'UBER',\n",
              " 'UDR',\n",
              " 'ULTA',\n",
              " 'UNP',\n",
              " 'UAL',\n",
              " 'UPS',\n",
              " 'URI',\n",
              " 'UNH',\n",
              " 'UHS',\n",
              " 'VLO',\n",
              " 'VTR',\n",
              " 'VLTO',\n",
              " 'VRSN',\n",
              " 'VRSK',\n",
              " 'VZ',\n",
              " 'VRTX',\n",
              " 'VTRS',\n",
              " 'VICI',\n",
              " 'V',\n",
              " 'VST',\n",
              " 'VMC',\n",
              " 'WRB',\n",
              " 'GWW',\n",
              " 'WAB',\n",
              " 'WBA',\n",
              " 'WMT',\n",
              " 'DIS',\n",
              " 'WBD',\n",
              " 'WM',\n",
              " 'WAT',\n",
              " 'WEC',\n",
              " 'WFC',\n",
              " 'WELL',\n",
              " 'WST',\n",
              " 'WDC',\n",
              " 'WY',\n",
              " 'WSM',\n",
              " 'WMB',\n",
              " 'WTW',\n",
              " 'WDAY',\n",
              " 'WYNN',\n",
              " 'XEL',\n",
              " 'XYL',\n",
              " 'YUM',\n",
              " 'ZBRA',\n",
              " 'ZBH',\n",
              " 'ZTS']"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "valid_tickers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91fqxQLRmQbD",
        "outputId": "b998a6ad-92b7-470a-b928-44ac49fa722f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scanning r/wallstreetbets...\n",
            "Scanning r/stocks...\n",
            "Scanning r/investing...\n",
            "\n",
            "Top 20 trending tickers detected on Reddit:\n",
            "A: 79 mentions\n",
            "ON: 69 mentions\n",
            "IT: 38 mentions\n",
            "UNH: 37 mentions\n",
            "ARE: 36 mentions\n",
            "T: 20 mentions\n",
            "ALL: 15 mentions\n",
            "NOW: 14 mentions\n",
            "DAY: 8 mentions\n",
            "NVDA: 7 mentions\n",
            "SO: 7 mentions\n",
            "HAS: 7 mentions\n",
            "TECH: 4 mentions\n",
            "GOOGL: 3 mentions\n",
            "D: 3 mentions\n",
            "GOOG: 3 mentions\n",
            "AMD: 3 mentions\n",
            "PLTR: 2 mentions\n",
            "LOW: 2 mentions\n",
            "RTX: 2 mentions\n",
            "\n",
            "Fetching posts for A...\n",
            "Fetched 15 posts for A\n",
            "\n",
            "Fetching posts for ON...\n",
            "Fetched 15 posts for ON\n",
            "\n",
            "Fetching posts for IT...\n",
            "Fetched 15 posts for IT\n",
            "\n",
            "Fetching posts for UNH...\n",
            "Fetched 15 posts for UNH\n",
            "\n",
            "Fetching posts for ARE...\n",
            "Fetched 15 posts for ARE\n",
            "\n",
            "Fetching posts for T...\n",
            "Fetched 15 posts for T\n",
            "\n",
            "Fetching posts for ALL...\n",
            "Fetched 15 posts for ALL\n",
            "\n",
            "Fetching posts for NOW...\n",
            "Fetched 15 posts for NOW\n",
            "\n",
            "Fetching posts for DAY...\n",
            "Fetched 15 posts for DAY\n",
            "\n",
            "Fetching posts for NVDA...\n",
            "Fetched 15 posts for NVDA\n",
            "\n",
            "Fetching posts for SO...\n",
            "Fetched 15 posts for SO\n",
            "\n",
            "Fetching posts for HAS...\n",
            "Fetched 15 posts for HAS\n",
            "\n",
            "Fetching posts for TECH...\n",
            "Fetched 15 posts for TECH\n",
            "\n",
            "Fetching posts for GOOGL...\n",
            "Fetched 15 posts for GOOGL\n",
            "\n",
            "Fetching posts for D...\n",
            "Fetched 15 posts for D\n",
            "\n",
            "Fetching posts for GOOG...\n",
            "Fetched 15 posts for GOOG\n",
            "\n",
            "Fetching posts for AMD...\n",
            "Fetched 15 posts for AMD\n",
            "\n",
            "Fetching posts for PLTR...\n",
            "Fetched 15 posts for PLTR\n",
            "\n",
            "Fetching posts for LOW...\n",
            "Fetched 15 posts for LOW\n",
            "\n",
            "Fetching posts for RTX...\n",
            "Fetched 15 posts for RTX\n",
            "Saved 300 posts to reddit_stock_posts.csv\n"
          ]
        }
      ],
      "source": [
        "import praw\n",
        "import re\n",
        "from collections import Counter\n",
        "import time\n",
        "import csv\n",
        "\n",
        "# Reddit API setup\n",
        "reddit = praw.Reddit(\n",
        "    client_id=\"Lk7amzjYlHw4NZN4jVeMOA\",\n",
        "    client_secret=\"4Xv7mFZcXHbcvUkQG98tMau-BvEFFg\",\n",
        "    user_agent=\"WSB_Sentiment_Bot/1.0 by Defiant-Fee-533\",\n",
        "    check_for_async=False\n",
        ")\n",
        "\n",
        "subreddits = ['wallstreetbets', 'stocks', 'investing']\n",
        "post_limit = 200  # number of posts to scan per subreddit\n",
        "\n",
        "# Regex to extract uppercase ticker symbols, allowing optional dot (like BRK.B)\n",
        "ticker_candidate_pattern = re.compile(r'\\b[A-Z]{1,5}(?:\\.[A-Z])?\\b')\n",
        "\n",
        "\n",
        "def find_trending_tickers():\n",
        "    all_candidates = []\n",
        "\n",
        "    for subreddit_name in subreddits:\n",
        "        print(f\"Scanning r/{subreddit_name}...\")\n",
        "        subreddit = reddit.subreddit(subreddit_name)\n",
        "        for post in subreddit.hot(limit=post_limit):\n",
        "            text = post.title.upper()\n",
        "            candidates = ticker_candidate_pattern.findall(text)\n",
        "            all_candidates.extend(candidates)\n",
        "            time.sleep(0.1)  # polite pause to avoid rate limit\n",
        "\n",
        "    # Filter candidates by whether they are valid tickers\n",
        "    filtered = [t for t in all_candidates if t in valid_tickers]\n",
        "\n",
        "    # Count frequencies and get top 20 trending tickers\n",
        "    counter = Counter(filtered)\n",
        "    top_20 = counter.most_common(20)\n",
        "\n",
        "    print(\"\\nTop 20 trending tickers detected on Reddit:\")\n",
        "    for ticker, count in top_20:\n",
        "        print(f\"{ticker}: {count} mentions\")\n",
        "\n",
        "    return [t[0] for t in top_20]\n",
        "\n",
        "def get_posts_for_ticker(ticker, max_posts=15):\n",
        "    posts = []\n",
        "    query = f'title:{ticker} OR selftext:{ticker}'\n",
        "\n",
        "    for subreddit_name in subreddits:\n",
        "        subreddit = reddit.subreddit(subreddit_name)\n",
        "        for post in subreddit.search(query, sort='new', limit=max_posts):\n",
        "            posts.append({\n",
        "                'ticker': ticker,\n",
        "                'post_id': post.id,\n",
        "                'title': post.title,\n",
        "                'selftext': post.selftext,\n",
        "                'url': post.url,\n",
        "                'created_utc': post.created_utc,\n",
        "                'score': post.score,\n",
        "                'num_comments': post.num_comments,\n",
        "                'subreddit': subreddit_name\n",
        "            })\n",
        "            if len(posts) >= max_posts:\n",
        "                break\n",
        "        if len(posts) >= max_posts:\n",
        "            break\n",
        "    return posts\n",
        "\n",
        "def save_posts_to_csv(posts, filename='reddit_stock_posts.csv'):\n",
        "    if not posts:\n",
        "        print(\"No posts to save.\")\n",
        "        return\n",
        "\n",
        "    keys = posts[0].keys()\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as output_file:\n",
        "        dict_writer = csv.DictWriter(output_file, fieldnames=keys)\n",
        "        dict_writer.writeheader()\n",
        "        dict_writer.writerows(posts)\n",
        "\n",
        "    print(f\"Saved {len(posts)} posts to {filename}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    trending_tickers = find_trending_tickers()\n",
        "\n",
        "    all_posts = []\n",
        "    for ticker in trending_tickers:\n",
        "        print(f\"\\nFetching posts for {ticker}...\")\n",
        "        posts = get_posts_for_ticker(ticker)\n",
        "        all_posts.extend(posts)\n",
        "        print(f\"Fetched {len(posts)} posts for {ticker}\")\n",
        "        time.sleep(1)\n",
        "\n",
        "    save_posts_to_csv(all_posts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvOetzJym7s-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
